


from dotenv import load_dotenv
load_dotenv("/home/lys202103652/.env")

import os

#HF_TOKEN = "get your token in http://hf.co/settings/tokens"
HF_TOKEN = os.getenv('HF_TOKEN')
print(HF_TOKEN)


from transformers import LlamaTokenizerFast, LlamaTokenizer

#tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
tokenizer = LlamaTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer")




tokenizer


encoded = tokenizer("Hi, good morning!!")
encoded


# decode를 원복을 해보면, "<s>"라는 특수 토큰이 포함되어 있다.  문자의 시작을 의미한다.
# 이런 특수토큰은, encode 방법에 따라, 다른 방식으로 표현된다.

decoded = tokenizer.decode(encoded['input_ids'])
decoded




for i in range(10):
    print(tokenizer(chr(i)))
    


vocab = tokenizer.get_vocab()



vocab


# 


%pip install llama-index-embeddings-huggingface
%pip install llama-index-embeddings-instructor


!pip install llama-index


from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# loads BAAI/bge-small-en
# embed_model = HuggingFaceEmbedding()

# loads BAAI/bge-small-en-v1.5
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")


embeddings = embed_model.get_text_embedding("Hello World!")

print(len(embeddings))
print(embeddings[:5])



