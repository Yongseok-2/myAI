
# chatbot template

# "test_llm_general.ipynb"을 기초로, chatbot query/answer를 진행하는 template



!pip install dotenv


from dotenv import load_dotenv
load_dotenv("/home/mhkwon/.env")

import os

#HF_TOKEN = "get your token in http://hf.co/settings/tokens"
HF_TOKEN = os.getenv('HF_TOKEN')
print(HF_TOKEN)

from huggingface_hub import login
hf_token = login(token=HF_TOKEN, add_to_git_credential=True)

# 에러가 나면, linux에서 다음 명령어를 실행
# git config --global credential.helper store


import torch

print(torch.__version__)


import transformers

print(transformers.__version__)


# 1) 표준 템플릿
# "test_llm_general.ipynb"에서 복사

#############################################################
# 0) 선언 부분

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# cpu/gpu를 선택 또는 지정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#device = "cpu"
print('Using device:', device)

model_id = 'meta-llama/Llama-3.2-1B' # 에러 발생
#model_id = 'LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct'
#model_id = 'naver-clova-ix/donut-base-finetuned-docvqa' # 에러 발생
#model_id = 'naver-clova-ix/donut-base-finetuned-cord-v2' # 에러 발생
#model_id = 'upstage/SOLAR-10.7B-v1.0'
#model_id = 'yanolja/EEVE-Korean-10.8B-v1.0'
#model_id = 'migtissera/Trinity-2-Codestral-22B-v0.2'
#model_id = 'BAAI/Infinity-Instruct-7M-Gen-Llama3_1-8B'


tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    #trust_remote_code=True,  # exaone only
)

#############################################################
# 1) prompt과정

# 결과값을 보여주는 template
if tokenizer.chat_template is None:
    tokenizer.chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}"

###########################################################
def my_aiquery(instruction):
    #instruction = "철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고, 민수가 남은 5개를 가져갔으면 철수에게 남은 연필의 갯수는 몇개인가요?"

    messages = [
        {"role": "user", "content": f"{instruction}"}
        ]
    
    
    #############################################################
    # 2) tokenizer과정
    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to(model.device)
    
    # 'unsloth/Llama-3.2-1B-Instruct 사용시에는 다음을 막아야 함.
    #if model_id == 'meta-llama/Llama-3.2-1B':
    #    model.generation_config.pad_token_id = model.generation_config.eos_token_id
    #    model.generation_config.pad_token_id = tokenizer.pad_token_id   # 설정하지 않으면, 무한 CPU 실행
    
    #if tokenizer.pad_token is None:
    #    tokenizer.pad_token = tokenizer.eos_token
    #if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id
    # 에러해결
    # The attention mask is not set and cannot be inferred from input because pad token is same as eos token. 
    # As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
    
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    
    #############################################################
    # 3) LLM과정
    
    # 실행시간을 측정하는 모듈
    import time
    
    start_time = time.time()
    outputs = model.generate(
        input_ids,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.6,
        top_p=0.9,
        #eos_token_id=terminators, 
        pad_token_id = tokenizer.eos_token_id,  # llama 3.2, bllossom
    )
    end_time = time.time()
    print('elapsed time =', end_time - start_time)
    
    
    #############################################################
    # 4) decoder과정
    answer = tokenizer.decode(outputs[0])

    # 특수 토근을 제거하고, 출력
    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
    print(response)

    return response





print("Let's chat! (type 'quit' to exit)")
while True:
    # sentence = "do you use credit cards?"
    sentence = input("You: ")
    if sentence == "quit":
        break
    response = my_aiquery(sentence)
    


# *** llama module로 나오는 코드는 피하라,

# 다음은 huggingface 방식으로 만든 template
# 위 소스에서 class으로 발전 시킨 모델


from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = 'meta-llama/Llama-3.2-1B' # 에러 발생
#model_id = "meta-llama/Llama-2-7b-chat-hf"

class ChatBot:
    def __init__(self):
        # 모델과 토크나이저 초기화
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto"
        )
    
    def generate_response(self, prompt, max_length=256):
        # 프롬프트 토큰화
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # 응답 생성
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # 응답 디코딩
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response

    def chat(self):
        print("챗봇과 대화를 시작합니다. (종료하려면 'quit' 입력)")
        while True:
            user_input = input("사용자: ")
            if user_input.lower() == 'quit':
                break
                
            response = self.generate_response(user_input)
            print(f"챗봇: {response}")

if __name__ == "__main__":
    bot = ChatBot()
    bot.chat()



