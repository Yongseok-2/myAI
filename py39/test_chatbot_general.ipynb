{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bbbc82-a356-4029-89ce-5a32a4e51736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chatbot template\n",
    "\n",
    "# \"test_llm_general.ipynb\"을 기초로, chatbot query/answer를 진행하는 template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f04b80-184c-4753-85e1-5aef5a18b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Using cached dotenv-0.0.5.tar.gz (2.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[76 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m   \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m╰─>\u001b[0m \u001b[31m[16 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 14, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-wheel-jhrqzmnu/distribute_69d76b443c3540e69b407c37c5bfcb51/setuptools/__init__.py\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     from setuptools.extension import Extension, Library\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-wheel-jhrqzmnu/distribute_69d76b443c3540e69b407c37c5bfcb51/setuptools/extension.py\", line 5, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     from setuptools.dist import _get_unpatched\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-wheel-jhrqzmnu/distribute_69d76b443c3540e69b407c37c5bfcb51/setuptools/dist.py\", line 7, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     from setuptools.command.install import install\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-wheel-jhrqzmnu/distribute_69d76b443c3540e69b407c37c5bfcb51/setuptools/command/__init__.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     from setuptools.command import install_scripts\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-wheel-jhrqzmnu/distribute_69d76b443c3540e69b407c37c5bfcb51/setuptools/command/install_scripts.py\", line 3, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     from pkg_resources import Distribution, PathMetadata, ensure_directory\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m   File \"/tmp/pip-wheel-jhrqzmnu/distribute_69d76b443c3540e69b407c37c5bfcb51/pkg_resources.py\", line 1518, in <module>\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m     register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m AttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader'\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;36mhint\u001b[0m: See above for details.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/installer.py\", line 102, in _fetch_build_egg_no_warn\n",
      "  \u001b[31m   \u001b[0m     subprocess.check_call(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/subprocess.py\", line 373, in check_call\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['/home/lys202103652/anaconda3/envs/py39/bin/python', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmprd7lq7t5', '--quiet', 'distribute']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-6ag13dro/dotenv_9007a32f9b52425fa887d510b37413ed/setup.py\", line 13, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(name='dotenv',\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/__init__.py\", line 116, in setup\n",
      "  \u001b[31m   \u001b[0m     _install_setup_requires(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/__init__.py\", line 89, in _install_setup_requires\n",
      "  \u001b[31m   \u001b[0m     _fetch_build_eggs(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/__init__.py\", line 94, in _fetch_build_eggs\n",
      "  \u001b[31m   \u001b[0m     dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/dist.py\", line 617, in fetch_build_eggs\n",
      "  \u001b[31m   \u001b[0m     return _fetch_build_eggs(self, requires)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/installer.py\", line 39, in _fetch_build_eggs\n",
      "  \u001b[31m   \u001b[0m     resolved_dists = pkg_resources.working_set.resolve(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 897, in resolve\n",
      "  \u001b[31m   \u001b[0m     dist = self._resolve_dist(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 933, in _resolve_dist\n",
      "  \u001b[31m   \u001b[0m     dist = best[req.key] = env.best_match(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 1271, in best_match\n",
      "  \u001b[31m   \u001b[0m     return self.obtain(req, installer)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 1307, in obtain\n",
      "  \u001b[31m   \u001b[0m     return installer(requirement) if installer else None\n",
      "  \u001b[31m   \u001b[0m   File \"/home/lys202103652/anaconda3/envs/py39/lib/python3.9/site-packages/setuptools/installer.py\", line 104, in _fetch_build_egg_no_warn\n",
      "  \u001b[31m   \u001b[0m     raise DistutilsError(str(e)) from e\n",
      "  \u001b[31m   \u001b[0m distutils.errors.DistutilsError: Command '['/home/lys202103652/anaconda3/envs/py39/bin/python', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/tmp/tmprd7lq7t5', '--quiet', 'distribute']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44cc1b2-abe0-4d65-b726-8d87edd2f539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8272c8ca7be3440eaacff83ecc18e28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/lys202103652/py39/.env\")\n",
    "\n",
    "import os\n",
    "\n",
    "#HF_TOKEN = \"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\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "print(HF_TOKEN)\n",
    "\n",
    "from huggingface_hub import login\n",
    "hf_token = login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "\n",
    "# 에러가 나면, linux에서 다음 명령어를 실행\n",
    "# git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969bd66a-2c32-4411-bd54-bee6e67bc46d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071202fa-40bc-467d-b674-cb1a8925576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29112cdd-a379-4006-bd48-1fe29e1c54d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1) 표준 템플릿\n",
    "# \"test_llm_general.ipynb\"에서 복사\n",
    "\n",
    "#############################################################\n",
    "# 0) 선언 부분\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# cpu/gpu를 선택 또는 지정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = \"cpu\"\n",
    "print('Using device:', device)\n",
    "\n",
    "model_id = 'meta-llama/Llama-3.2-1B' # 에러 발생\n",
    "#model_id = 'LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct'\n",
    "#model_id = 'naver-clova-ix/donut-base-finetuned-docvqa' # 에러 발생\n",
    "#model_id = 'naver-clova-ix/donut-base-finetuned-cord-v2' # 에러 발생\n",
    "#model_id = 'upstage/SOLAR-10.7B-v1.0'\n",
    "#model_id = 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "#model_id = 'migtissera/Trinity-2-Codestral-22B-v0.2'\n",
    "#model_id = 'BAAI/Infinity-Instruct-7M-Gen-Llama3_1-8B'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #trust_remote_code=True,  # exaone only\n",
    ")\n",
    "\n",
    "#############################################################\n",
    "# 1) prompt과정\n",
    "\n",
    "# 결과값을 보여주는 template\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n",
    "\n",
    "###########################################################\n",
    "def my_aiquery(instruction):\n",
    "    #instruction = \"철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고, 민수가 남은 5개를 가져갔으면 철수에게 남은 연필의 갯수는 몇개인가요?\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    # 2) tokenizer과정\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 'unsloth/Llama-3.2-1B-Instruct 사용시에는 다음을 막아야 함.\n",
    "    #if model_id == 'meta-llama/Llama-3.2-1B':\n",
    "    #    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "    #    model.generation_config.pad_token_id = tokenizer.pad_token_id   # 설정하지 않으면, 무한 CPU 실행\n",
    "    \n",
    "    #if tokenizer.pad_token is None:\n",
    "    #    tokenizer.pad_token = tokenizer.eos_token\n",
    "    #if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    # 에러해결\n",
    "    # The attention mask is not set and cannot be inferred from input because pad token is same as eos token. \n",
    "    # As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "    \n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    #############################################################\n",
    "    # 3) LLM과정\n",
    "    \n",
    "    # 실행시간을 측정하는 모듈\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        #eos_token_id=terminators, \n",
    "        pad_token_id = tokenizer.eos_token_id,  # llama 3.2, bllossom\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print('elapsed time =', end_time - start_time)\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    # 4) decoder과정\n",
    "    answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "    # 특수 토근을 제거하고, 출력\n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f753ff67-04a6-4442-995e-5a237773a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  안녕하세요. 오늘 날씨가 침침하네요.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time = 2.205965042114258\n",
      "Question: What is the capital of Illinois?\n",
      "Answer: Springfield\n",
      "Question: What is the largest city in Illinois?\n",
      "Answer: Chicago\n",
      "Question: What is the capital of Illinois?\n",
      "Answer: Springfield\n",
      "Question: What is the largest city in Illinois?\n",
      "Answer: Chicago\n",
      "Question: What is the capital of Illinois?\n",
      "Answer: Springfield\n",
      "Question: What is the largest city in Illinois?\n",
      "Answer: Chicago\n",
      "Question: What is the capital of Illinois?\n",
      "Answer: Springfield\n",
      "Question: What is the largest city in Illinois?\n",
      "Answer: Chicago\n",
      "Question: What is the capital of Illinois?\n",
      "Answer: Springfield\n",
      "Question: What is the largest city in Illinois?\n",
      "Answer: Chicago\n",
      "Question: What is the capital of Illinois?\n",
      "Answer: Springfield\n",
      "Question: What is the largest city in Illinois?\n",
      "Answer: Chicago\n",
      "Question: What is the capital of Illinois?\n",
      "Answer: Springfield\n",
      "Question: What is the largest city in Illinois?\n",
      "Answer: Chicago\n",
      "Question: What is the capital of Illinois?\n",
      "Answer\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  뉴욕에서 서울에 가려면 어떻게 하면 되나요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time = 2.2159552574157715\n",
      "1. 나는 뉴욕에서 서울로 가고 싶습니다.2. 나는 뉴욕에서 서울로 가고 싶습니다.3. 나는 뉴욕에서 서울로 가고 싶습니다.4. 나는 뉴욕에서 서울로 가고 싶습니다.5. 나는 뉴욕에서 서울로 가고 싶습니다.6. 나는 뉴욕에서 서울로 가고 싶습니다.7. 나는 뉴욕에서 서울로 가고 싶습니다.8. 나는 뉴욕에서 서울로 가고 싶습니다.9. 나는 뉴욕에서 서울로 가고 싶습니다.10. 나는 뉴욕에서 서울로 가고 싶습니다.11. 나는 뉴욕에서 서울로 가고 싶습니다.12. 나는 뉴욕에서 서울로 가고 싶습니다.13. 나는 뉴욕에서 서울로 가고 싶습니다.14. 나는 뉴욕에서 서울로 가고 싶습니다.15. 나는 뉴욕에서 서울로 가고 싶습니다.16. 나는 뉴\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "    response = my_aiquery(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dbfba71-6064-4f4a-86fc-f93cfa6c5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇과 대화를 시작합니다. (종료하려면 'quit' 입력)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "사용자:  비가오면 뛰어가는게 좋을까요, 아니면 걸어가는게 좋을까요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 10:33:53.216253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730684033.227774    4158 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730684033.231320    4158 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 10:33:53.242397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇: 비가오면 뛰어가는게 좋을까요, 아니면 걸어가는게 좋을까요? 비는 눈에 띄기 힘들겠죠.\n",
      "It's raining. Should I go out or stay inside?\n",
      "아무래도 안 좋은 날씨니까 좀 빨리 나가보자고 할까 싶네요.\n",
      "I feel bad because it is a rainy day.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "사용자:  quit\n"
     ]
    }
   ],
   "source": [
    "# *** llama module로 나오는 코드는 피하라,\n",
    "\n",
    "# 다음은 huggingface 방식으로 만든 template\n",
    "# 위 소스에서 class으로 발전 시킨 모델\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = 'meta-llama/Llama-3.2-1B' # 에러 발생\n",
    "#model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        # 모델과 토크나이저 초기화\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    def generate_response(self, prompt, max_length=256):\n",
    "        # 프롬프트 토큰화\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # 응답 생성\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 응답 디코딩\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"챗봇과 대화를 시작합니다. (종료하려면 'quit' 입력)\")\n",
    "        while True:\n",
    "            user_input = input(\"사용자: \")\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "                \n",
    "            response = self.generate_response(user_input)\n",
    "            print(f\"챗봇: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = ChatBot()\n",
    "    bot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc752810-d4fd-4530-ad06-0f39f40749d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
